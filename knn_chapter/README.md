### 模型原理

K-近邻分类器是一种基于实例的分类算法，其核心思想是利用样本间的相似性进行分类，遵循“物以类聚”的原则。

该算法的基本原理如下：

1. **惰性学习特性**  
   作为惰性学习算法，KNN在训练阶段不进行模型参数的学习或计算，仅存储全部训练样本的特征和对应的类别标签。模型的核心计算过程延迟到预测阶段进行。

2. **距离度量**  
   采用欧氏距离衡量样本间的相似性。对于两个d维特征样本$x=(x_1,x_2,...,x_d)$和$y=(y_1,y_2,...,y_d)$，欧氏距离计算公式为：  
   $$distance(x,y) = \sqrt{\sum_{i=1}^{d}(x_i - y_i)^2}$$  
   距离越小，两个样本的相似性越高。

3. **近邻选择**  
   对于待分类的测试样本，计算其与所有训练样本的欧氏距离，然后根据距离从小到大排序，选取距离最近的K个训练样本作为“近邻”（K为预先设定的正整数）。

4. **多数投票决策**  
   统计K个近邻的类别标签，出现次数最多的类别即为该测试样本的预测类别，通过多数投票机制确定最终分类结果。

5. **K值的影响**  
   K值是关键参数：K值过小，模型易受噪声影响，可能导致过拟合；K值过大，可能包含过多其他类别的样本，导致分类模糊，可能引发欠拟合。实际应用中需根据具体问题选择合适的K值。

### 代码实现解析
**`KNN`类初始化（`__init__`方法）**  
`KNN.py`和`test.py`中，初始化方法均接收`n_neighbors`参数，`KNN.py`会校验其是否为正整数（非正则抛`ValueError`），两者都初始化`_X_train`和`_y_train`为`None`以存储训练数据。

**训练方法（`fit`方法）**  
两个文件的`fit`方法均体现KNN惰性学习特性，仅将输入的`X_train`和`y_train`分别赋值给`_X_train`和`_y_train`，不做额外计算，返回实例本身支持链式调用。

**预测方法（`predict`方法）**  
先验证模型是否训练、测试数据是否为空及`n_neighbors`是否小于训练样本数（不满足则抛异常），然后遍历每个测试样本，用`np.linalg.norm`计算与所有训练样本的欧氏距离，通过`np.argsort`取前K个近邻索引，提取标签后用`stats.mode`多数投票确定预测类别，最终返回预测结果数组；通过列表推导式计算所有测试样本与训练样本的欧氏距离，排序后取前K个近邻索引，提取标签并用`np.argmax(np.bincount)`进行多数投票，返回预测结果数组。

**主函数（`main`方法）**  
创建简单二维二分类样本，演示模型初始化、训练、预测流程，输出数据形状、预测结果、运行时间及结果验证；加载鸢尾花数据集，拆分训练集（80%）和测试集（20%），用K=3的模型预测，输出运行时间、准确率、混淆矩阵及分类报告等评估指标。

### 实验结果
`KNN.py`在简单二维二分类任务中，模型以K=3完成预测，测试样本预测类别与预期完全匹配，且运行时间仅0.001005秒。从结果可分析：一方面，在特征维度低、样本数量少的简单场景下，KNN无需复杂训练即可快速捕捉样本间的距离关系，分类逻辑精准落地；另一方面，较小的K值（K=3）能有效聚焦局部相似样本，避免无关样本干扰，保证了简单任务的分类准确性。  
![knn](https://i.hd-r.cn/929937c0296a15ae7007aa57d82ca4cf.png)
`test.py`在鸢尾花多分类任务中，模型运行时间0.004010秒，准确率、各类别精确率/召回率/F1分数均达1.0，混淆矩阵无错分样本。该结果表明：其一，鸢尾花数据集特征区分度高，欧氏距离能有效衡量样本相似性，适配KNN的距离度量逻辑；其二，K=3的参数选择在该数据集上平衡了局部相似性与类别代表性，既避免了K值过小导致的噪声敏感问题，也未因K值过大引入其他类别样本，最终实现完美分类，验证了手搓KNN在经典结构化数据集上的可靠性与适配性。
![test](https://i.hd-r.cn/c1cb32e58af8f170ec0faa1a252c1f01.png)